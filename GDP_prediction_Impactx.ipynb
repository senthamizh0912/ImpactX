{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2XmW20jNPzT",
        "outputId": "f0897e51-1be2-41ad-dbad-b8fe30ba4e37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.23.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.29.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.23.3-py3-none-any.whl (46.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.23.3 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.3 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install scikit-learn\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "RYI5GbAkZQKa",
        "outputId": "81fc073f-96a3-41d0-9d6f-207e07a7f654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://299ce1c5559a503430.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://299ce1c5559a503430.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#GITHUB REPOSITORY LINK : https://github.com/Joelrajjoe/ImapctX-GDP-.git\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import gradio as gr\n",
        "import io\n",
        "import base64\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Global variables to store data between function calls\n",
        "global_forecast_df = None\n",
        "global_ts_df = None\n",
        "\n",
        "# Function to load and preprocess the data\n",
        "def load_data(file_obj):\n",
        "    \"\"\"\n",
        "    Load the data from the uploaded file and perform initial preprocessing.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the uploaded file as a CSV\n",
        "        df = pd.read_csv(file_obj.name)\n",
        "        print(f\"Data loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Extract GDP columns from the dataframe\n",
        "def extract_gdp_columns(df):\n",
        "    \"\"\"\n",
        "    Extract columns related to GDP per capita from the dataframe.\n",
        "    \"\"\"\n",
        "    gdp_columns = [col for col in df.columns if 'NSDP Per Capita (Nominal)' in col]\n",
        "    return gdp_columns\n",
        "\n",
        "# Function to extract year from column name\n",
        "def extract_year(col_name):\n",
        "    \"\"\"\n",
        "    Extract the year from the column name.\n",
        "    \"\"\"\n",
        "    year_str = col_name.split(')')[-1]\n",
        "    if '-' in year_str:\n",
        "        start_year = int(year_str.split('-')[0])\n",
        "        return start_year\n",
        "    else:\n",
        "        # If the format is different, try to extract the year another way\n",
        "        return int(year_str)\n",
        "\n",
        "# Function to clean GDP values\n",
        "def clean_gdp_value(value):\n",
        "    \"\"\"\n",
        "    Convert GDP string values to float by removing non-numeric characters.\n",
        "    \"\"\"\n",
        "    if isinstance(value, str):\n",
        "        # Remove ₹ symbol and any commas, spaces, etc.\n",
        "        cleaned = ''.join(c for c in value if c.isdigit() or c == '.')\n",
        "        try:\n",
        "            return float(cleaned)\n",
        "        except ValueError:\n",
        "            return np.nan\n",
        "    return value\n",
        "\n",
        "# Prepare time series data with proper cleaning\n",
        "def prepare_time_series_data(df, gdp_columns):\n",
        "    \"\"\"\n",
        "    Transform the data for time series analysis with proper data cleaning.\n",
        "    \"\"\"\n",
        "    ts_data = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        state = row['State Name']\n",
        "        for col in gdp_columns:\n",
        "            year = extract_year(col)\n",
        "            gdp = clean_gdp_value(row[col])  # Clean the GDP value\n",
        "            ts_data.append({'State': state, 'Year': year, 'GDP_Per_Capita': gdp})\n",
        "\n",
        "    ts_df = pd.DataFrame(ts_data)\n",
        "    # Drop any rows with NaN GDP values\n",
        "    ts_df = ts_df.dropna(subset=['GDP_Per_Capita'])\n",
        "    return ts_df\n",
        "\n",
        "# Fixed feature engineering function\n",
        "def create_features(df):\n",
        "    \"\"\"\n",
        "    Create features for time series forecasting.\n",
        "    \"\"\"\n",
        "    # Create a copy of the dataframe to avoid modifying the original\n",
        "    df_features = df.copy()\n",
        "\n",
        "    # Group by state\n",
        "    for state in df_features['State'].unique():\n",
        "        state_mask = df_features['State'] == state\n",
        "        df_state = df_features[state_mask].sort_values('Year')\n",
        "\n",
        "        # Create lag features (only if enough data)\n",
        "        if len(df_state) > 2:\n",
        "            df_features.loc[state_mask, 'GDP_Lag1'] = df_state['GDP_Per_Capita'].shift(1)\n",
        "            df_features.loc[state_mask, 'GDP_Lag2'] = df_state['GDP_Per_Capita'].shift(2)\n",
        "\n",
        "            # Create growth rate feature\n",
        "            df_features.loc[state_mask, 'Growth_Rate'] = df_state['GDP_Per_Capita'].pct_change()\n",
        "\n",
        "            # Create the lagged growth rate\n",
        "            df_features.loc[state_mask, 'Growth_Rate_Lag1'] = df_features.loc[state_mask, 'Growth_Rate'].shift(1)\n",
        "\n",
        "            # Add rolling averages (only if enough data)\n",
        "            df_features.loc[state_mask, 'Rolling_Avg_3Y'] = df_state['GDP_Per_Capita'].rolling(window=min(3, len(df_state)), min_periods=1).mean()\n",
        "\n",
        "    # Drop rows with NaN (first two years for each state)\n",
        "    df_features = df_features.dropna()\n",
        "\n",
        "    # Create state encoding\n",
        "    df_features = pd.get_dummies(df_features, columns=['State'], drop_first=True)\n",
        "\n",
        "    return df_features\n",
        "\n",
        "# Function to calculate Gini coefficient\n",
        "def calculate_gini(array):\n",
        "    \"\"\"\n",
        "    Calculate the Gini coefficient of inequality\n",
        "    \"\"\"\n",
        "    array = np.array(array)\n",
        "    if len(array) < 2:\n",
        "        return 0  # Need at least 2 points to calculate inequality\n",
        "\n",
        "    # Sort array\n",
        "    array = np.sort(array)\n",
        "    # Calculate cumulative sum of array\n",
        "    cumulative_sum = np.cumsum(array)\n",
        "    # Calculate cumulative share of population and income\n",
        "    n = len(array)\n",
        "    cumulative_people = np.arange(1, n + 1) / n\n",
        "    cumulative_income = cumulative_sum / cumulative_sum[-1]\n",
        "    # Calculate Gini coefficient\n",
        "    gini = 1 - 2 * np.sum((cumulative_income[:-1] + cumulative_income[1:]) / 2 * np.diff(cumulative_people))\n",
        "    return gini\n",
        "\n",
        "# Function to forecast future GDP\n",
        "def forecast_future_gdp(best_model, scaler, feature_df, ts_df, forecast_years=3):\n",
        "    \"\"\"\n",
        "    Generate GDP forecasts for future years using the trained model.\n",
        "    \"\"\"\n",
        "    # Get the latest data\n",
        "    latest_year = ts_df['Year'].max()\n",
        "    future_predictions = {}\n",
        "\n",
        "    # Prepare data for each state\n",
        "    for state in ts_df['State'].unique():\n",
        "        state_data = ts_df[ts_df['State'] == state].sort_values('Year')\n",
        "\n",
        "        # Check if we have enough data for this state\n",
        "        if len(state_data) < 3:\n",
        "            continue\n",
        "\n",
        "        # Get the latest values for this state\n",
        "        latest_state_data = state_data[state_data['Year'] == latest_year]\n",
        "        if len(latest_state_data) == 0:\n",
        "            continue\n",
        "\n",
        "        current_gdp = latest_state_data['GDP_Per_Capita'].values[0]\n",
        "        state_predictions = [current_gdp]\n",
        "\n",
        "        # Create a prediction for each future year\n",
        "        for i in range(1, forecast_years + 1):\n",
        "            future_year = latest_year + i\n",
        "\n",
        "            # Create a feature row similar to our training data\n",
        "            pred_features = pd.DataFrame({\n",
        "                'GDP_Lag1': [state_predictions[-1]],\n",
        "                'GDP_Lag2': [state_data.iloc[-2]['GDP_Per_Capita'] if i == 1 else state_predictions[-2]],\n",
        "                'Growth_Rate': [(state_predictions[-1] /\n",
        "                                (state_data.iloc[-2]['GDP_Per_Capita'] if i == 1 else state_predictions[-2]) - 1)],\n",
        "                'Growth_Rate_Lag1': [(state_data.iloc[-1]['GDP_Per_Capita'] / state_data.iloc[-2]['GDP_Per_Capita'] - 1)\n",
        "                                    if i == 1 else (state_predictions[-2] / state_predictions[-3] - 1) if i > 2 else 0],\n",
        "                'Rolling_Avg_3Y': [np.mean(state_predictions[-3:] if i > 2 else\n",
        "                                          state_predictions + [state_data.iloc[-(3-len(state_predictions))]['GDP_Per_Capita']\n",
        "                                                           for _ in range(3-len(state_predictions))])]\n",
        "            })\n",
        "\n",
        "            # Add state one-hot encoding\n",
        "            for s in ts_df['State'].unique():\n",
        "                if s != list(ts_df['State'].unique())[0]:  # Skip the reference state\n",
        "                    col_name = f'State_{s}'\n",
        "                    pred_features[col_name] = [1 if s == state else 0]\n",
        "\n",
        "            # Fill in any missing columns from training data\n",
        "            for col in feature_df.drop(['GDP_Per_Capita', 'Year'], axis=1).columns:\n",
        "                if col not in pred_features.columns:\n",
        "                    pred_features[col] = 0\n",
        "\n",
        "            # Ensure columns are in the same order as training data\n",
        "            pred_features = pred_features[feature_df.drop(['GDP_Per_Capita', 'Year'], axis=1).columns]\n",
        "\n",
        "            # Scale the features\n",
        "            pred_features_scaled = scaler.transform(pred_features)\n",
        "\n",
        "            # Make prediction\n",
        "            pred_gdp = best_model.predict(pred_features_scaled)[0]\n",
        "            state_predictions.append(pred_gdp)\n",
        "\n",
        "        # Store predictions for this state\n",
        "        future_predictions[state] = {\n",
        "            latest_year + i: state_predictions[i] for i in range(1, forecast_years + 1)\n",
        "        }\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    forecast_df = pd.DataFrame(columns=['State', 'Year', 'Forecasted_GDP'])\n",
        "    for state, years_dict in future_predictions.items():\n",
        "        for year, gdp in years_dict.items():\n",
        "            forecast_df = pd.concat([forecast_df, pd.DataFrame({\n",
        "                'State': [state],\n",
        "                'Year': [year],\n",
        "                'Forecasted_GDP': [gdp]\n",
        "            })], ignore_index=True)\n",
        "\n",
        "    return forecast_df\n",
        "\n",
        "# Create visualizations for the UI\n",
        "def create_visualizations(ts_df, y_test=None, y_pred=None, best_model_name=None, forecast_df=None):\n",
        "    \"\"\"Generate visualizations and return them as base64 encoded images\"\"\"\n",
        "    images = {}\n",
        "\n",
        "    # 1. GDP Trends for Top States\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    latest_year = ts_df['Year'].max()\n",
        "    top_states = ts_df[ts_df['Year'] == latest_year].nlargest(5, 'GDP_Per_Capita')['State'].unique()\n",
        "    top_states_data = ts_df[ts_df['State'].isin(top_states)]\n",
        "\n",
        "    sns.lineplot(data=top_states_data, x='Year', y='GDP_Per_Capita', hue='State')\n",
        "    plt.title('GDP Per Capita Trends for Top 5 States')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('GDP Per Capita')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    buf.seek(0)\n",
        "    images['top_states_trend'] = base64.b64encode(buf.read()).decode('utf-8')\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Inequality Analysis (Gini Coefficient)\n",
        "    yearly_inequality = {}\n",
        "    for year in sorted(ts_df['Year'].unique()):\n",
        "        year_data = ts_df[ts_df['Year'] == year]['GDP_Per_Capita']\n",
        "        if len(year_data) > 1:  # Need at least 2 states to calculate inequality\n",
        "            yearly_inequality[year] = {\n",
        "                'Gini': calculate_gini(year_data),\n",
        "                'Max/Min Ratio': year_data.max() / year_data.min() if year_data.min() > 0 else np.nan,\n",
        "                'Standard Deviation': year_data.std(),\n",
        "                'Coefficient of Variation': year_data.std() / year_data.mean() if year_data.mean() > 0 else np.nan\n",
        "            }\n",
        "\n",
        "    inequality_df = pd.DataFrame(yearly_inequality).T\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(inequality_df.index, inequality_df['Gini'], marker='o')\n",
        "    plt.title('Gini Coefficient Over Time (Higher = More Inequality)')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Gini Coefficient')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    buf.seek(0)\n",
        "    images['inequality_trend'] = base64.b64encode(buf.read()).decode('utf-8')\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Growth Rate Analysis\n",
        "    if 'Coefficient of Variation' in inequality_df.columns:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(inequality_df.index, inequality_df['Coefficient of Variation'], marker='o', color='green')\n",
        "        plt.title('Coefficient of Variation Over Time (Higher = More Disparity)')\n",
        "        plt.xlabel('Year')\n",
        "        plt.ylabel('Coefficient of Variation')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        plt.savefig(buf, format='png')\n",
        "        buf.seek(0)\n",
        "        images['cv_trend'] = base64.b64encode(buf.read()).decode('utf-8')\n",
        "        plt.close()\n",
        "\n",
        "    # 4. Model Performance Visualization (if available)\n",
        "    if y_test is not None and y_pred is not None and best_model_name is not None:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "        min_val = min(min(y_test), min(y_pred))\n",
        "        max_val = max(max(y_test), max(y_pred))\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
        "        plt.xlabel('Actual GDP Per Capita')\n",
        "        plt.ylabel('Predicted GDP Per Capita')\n",
        "        plt.title(f'Actual vs. Predicted GDP Per Capita using {best_model_name}')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        plt.savefig(buf, format='png')\n",
        "        buf.seek(0)\n",
        "        images['model_performance'] = base64.b64encode(buf.read()).decode('utf-8')\n",
        "        plt.close()\n",
        "\n",
        "    # 5. Forecast Visualization (if available)\n",
        "    if forecast_df is not None and len(forecast_df) > 0:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        for state in top_states:\n",
        "            # Historical data\n",
        "            hist_data = ts_df[ts_df['State'] == state]\n",
        "            plt.plot(hist_data['Year'], hist_data['GDP_Per_Capita'], marker='o', label=f\"{state} (Historical)\")\n",
        "\n",
        "            # Forecast data\n",
        "            fc_data = forecast_df[forecast_df['State'] == state]\n",
        "            if not fc_data.empty:\n",
        "                plt.plot(fc_data['Year'], fc_data['Forecasted_GDP'], marker='*', linestyle='--',\n",
        "                         label=f\"{state} (Forecast)\")\n",
        "\n",
        "        plt.title('GDP Per Capita Forecast for Top 5 States')\n",
        "        plt.xlabel('Year')\n",
        "        plt.ylabel('GDP Per Capita')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        plt.savefig(buf, format='png')\n",
        "        buf.seek(0)\n",
        "        images['forecast'] = base64.b64encode(buf.read()).decode('utf-8')\n",
        "        plt.close()\n",
        "\n",
        "    return images\n",
        "\n",
        "# Run analysis function - core function that processes the data and builds models\n",
        "def run_analysis(file):\n",
        "    global global_forecast_df, global_ts_df\n",
        "\n",
        "    try:\n",
        "        # Load data from uploaded file\n",
        "        df = load_data(file)\n",
        "        if df is None:\n",
        "            return \"Error loading data file\", None, None, None, None, None, None\n",
        "\n",
        "        # Extract GDP columns\n",
        "        gdp_columns = extract_gdp_columns(df)\n",
        "        if not gdp_columns:\n",
        "            return \"No GDP columns found in the data\", None, None, None, None, None, None\n",
        "\n",
        "        # Prepare time series data\n",
        "        ts_df = prepare_time_series_data(df, gdp_columns)\n",
        "        if len(ts_df) == 0:\n",
        "            return \"Could not prepare time series data\", None, None, None, None, None, None\n",
        "\n",
        "        # Store the original time series data globally\n",
        "        global_ts_df = ts_df\n",
        "\n",
        "        # Create features for modeling\n",
        "        try:\n",
        "            feature_df = create_features(ts_df)\n",
        "            if len(feature_df) == 0:\n",
        "                return \"Not enough time series data for feature creation\", None, None, None, None, None, None\n",
        "        except Exception as e:\n",
        "            return f\"Error in feature creation: {str(e)}\", None, None, None, None, None, None\n",
        "\n",
        "        # Prepare data for modeling\n",
        "        X = feature_df.drop(['GDP_Per_Capita', 'Year'], axis=1)\n",
        "        y = feature_df['GDP_Per_Capita']\n",
        "\n",
        "        # Split data for training and testing\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Scale the features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # Define models\n",
        "        models = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'Ridge Regression': Ridge(alpha=1.0),\n",
        "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "        }\n",
        "\n",
        "        # Evaluate models using cross-validation\n",
        "        results = {}\n",
        "        tscv = TimeSeriesSplit(n_splits=min(5, len(X_train) // 2))\n",
        "\n",
        "        model_results = \"\"\n",
        "        for name, model in models.items():\n",
        "            try:\n",
        "                cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=tscv,\n",
        "                                          scoring='neg_mean_squared_error')\n",
        "                rmse_scores = np.sqrt(-cv_scores)\n",
        "                results[name] = {\n",
        "                    'cv_rmse_mean': rmse_scores.mean(),\n",
        "                    'cv_rmse_std': rmse_scores.std()\n",
        "                }\n",
        "                model_results += f\"{name} - CV RMSE: {rmse_scores.mean():.2f} (+/- {rmse_scores.std():.2f})\\n\"\n",
        "            except Exception as e:\n",
        "                model_results += f\"Error evaluating {name}: {str(e)}\\n\"\n",
        "\n",
        "        if not results:\n",
        "            return \"No models could be trained successfully\", None, None, None, None, None, None\n",
        "\n",
        "        # Train the best model\n",
        "        best_model_name = min(results, key=lambda x: results[x]['cv_rmse_mean'])\n",
        "        best_model = models[best_model_name]\n",
        "\n",
        "        best_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = best_model.predict(X_test_scaled)\n",
        "\n",
        "        # Calculate metrics\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        metrics = f\"Best Model: {best_model_name}\\n\"\n",
        "        metrics += f\"Mean Squared Error (MSE): {mse:.2f}\\n\"\n",
        "        metrics += f\"Root Mean Squared Error (RMSE): {rmse:.2f}\\n\"\n",
        "        metrics += f\"Mean Absolute Error (MAE): {mae:.2f}\\n\"\n",
        "        metrics += f\"R² Score: {r2:.4f}\"\n",
        "\n",
        "        # Generate future forecasts\n",
        "        forecast_df = forecast_future_gdp(best_model, scaler, feature_df, ts_df)\n",
        "\n",
        "        # Store forecast data globally\n",
        "        global_forecast_df = forecast_df\n",
        "\n",
        "        # Create visualizations\n",
        "        viz_images = create_visualizations(ts_df, y_test, y_pred, best_model_name, forecast_df)\n",
        "\n",
        "        # Feature importance\n",
        "        feature_importance = None\n",
        "        if hasattr(best_model, 'feature_importances_'):\n",
        "            feature_importance = pd.DataFrame({\n",
        "                'Feature': X.columns,\n",
        "                'Importance': best_model.feature_importances_\n",
        "            }).sort_values('Importance', ascending=False)\n",
        "\n",
        "        # Social impact analysis\n",
        "        yearly_inequality = {}\n",
        "        for year in sorted(ts_df['Year'].unique()):\n",
        "            year_data = ts_df[ts_df['Year'] == year]['GDP_Per_Capita']\n",
        "            if len(year_data) > 1:\n",
        "                yearly_inequality[year] = {\n",
        "                    'Gini': calculate_gini(year_data),\n",
        "                    'Max/Min Ratio': year_data.max() / year_data.min() if year_data.min() > 0 else np.nan,\n",
        "                    'Standard Deviation': year_data.std(),\n",
        "                    'Coefficient of Variation': year_data.std() / year_data.mean() if year_data.mean() > 0 else np.nan\n",
        "                }\n",
        "\n",
        "        inequality_df = pd.DataFrame(yearly_inequality).T\n",
        "\n",
        "        social_impact = \"Economic Inequality Analysis:\\n\\n\"\n",
        "        if not inequality_df.empty:\n",
        "            social_impact += f\"Starting Gini Coefficient (earliest year): {inequality_df['Gini'].iloc[0]:.4f}\\n\"\n",
        "            social_impact += f\"Ending Gini Coefficient (latest year): {inequality_df['Gini'].iloc[-1]:.4f}\\n\"\n",
        "\n",
        "            if inequality_df['Gini'].iloc[-1] > inequality_df['Gini'].iloc[0]:\n",
        "                social_impact += \"Trend: Inequality has INCREASED over the observed period.\\n\\n\"\n",
        "            else:\n",
        "                social_impact += \"Trend: Inequality has DECREASED over the observed period.\\n\\n\"\n",
        "\n",
        "        social_impact += \"States with highest predicted growth rates:\\n\"\n",
        "        growth_predictions = {}\n",
        "\n",
        "        for state in forecast_df['State'].unique():\n",
        "            state_data = forecast_df[forecast_df['State'] == state].sort_values('Year')\n",
        "            if len(state_data) >= 2:\n",
        "                first_year = state_data['Forecasted_GDP'].iloc[0]\n",
        "                last_year = state_data['Forecasted_GDP'].iloc[-1]\n",
        "                growth_rate = (last_year / first_year - 1) * 100\n",
        "                growth_predictions[state] = growth_rate\n",
        "\n",
        "        # Sort by growth rate and get top 5\n",
        "        top_growth_states = sorted(growth_predictions.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "        for state, growth in top_growth_states:\n",
        "            social_impact += f\"- {state}: {growth:.2f}%\\n\"\n",
        "\n",
        "        social_impact += \"\\nPolicy Implications:\\n\"\n",
        "        social_impact += \"1. States with lower GDP per capita require targeted development initiatives\\n\"\n",
        "        social_impact += \"2. Focus on reducing inequality through inclusive growth strategies\\n\"\n",
        "        social_impact += \"3. Implement policies to boost growth in states with poor forecast projections\\n\"\n",
        "\n",
        "        return \"Analysis complete\", model_results, metrics, viz_images, feature_importance, social_impact, forecast_df\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        return f\"Error in analysis: {str(e)}\\n{traceback.format_exc()}\", None, None, None, None, None, None\n",
        "\n",
        "# State prediction function for the UI\n",
        "def predict_state(state_name, forecast_df):\n",
        "    \"\"\"Get predictions for a specific state\"\"\"\n",
        "    if forecast_df is None or len(forecast_df) == 0:\n",
        "        return \"No forecast data available. Please run the analysis first.\"\n",
        "\n",
        "    if state_name not in forecast_df['State'].unique():\n",
        "        return f\"No data available for {state_name}. Available states: {', '.join(forecast_df['State'].unique())}\"\n",
        "\n",
        "    state_forecast = forecast_df[forecast_df['State'] == state_name].sort_values('Year')\n",
        "\n",
        "    result = f\"GDP Per Capita Forecast for {state_name}:\\n\\n\"\n",
        "    for _, row in state_forecast.iterrows():\n",
        "        result += f\"Year {int(row['Year'])}: ₹{row['Forecasted_GDP']:,.2f}\\n\"\n",
        "\n",
        "    return result\n",
        "\n",
        "# Get states with highest and lowest projected growth\n",
        "def get_growth_analysis(forecast_df):\n",
        "    \"\"\"Analyze growth rates from forecast data\"\"\"\n",
        "    if forecast_df is None or len(forecast_df) == 0:\n",
        "        return \"No forecast data available. Please run the analysis first.\"\n",
        "\n",
        "    growth_rates = {}\n",
        "    for state in forecast_df['State'].unique():\n",
        "        state_data = forecast_df[forecast_df['State'] == state].sort_values('Year')\n",
        "        if len(state_data) >= 2:\n",
        "            first_gdp = state_data['Forecasted_GDP'].iloc[0]\n",
        "            last_gdp = state_data['Forecasted_GDP'].iloc[-1]\n",
        "            growth = (last_gdp / first_gdp - 1) * 100\n",
        "            growth_rates[state] = growth\n",
        "\n",
        "    # Get top 5 and bottom 5 states by growth rate\n",
        "    sorted_states = sorted(growth_rates.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_states = sorted_states[:min(5, len(sorted_states))]\n",
        "    bottom_states = sorted_states[-min(5, len(sorted_states)):]\n",
        "\n",
        "    result = \"States with Highest Projected Growth Rates:\\n\\n\"\n",
        "    for state, rate in top_states:\n",
        "        result += f\"{state}: {rate:.2f}%\\n\"\n",
        "\n",
        "    result += \"\\nStates with Lowest Projected Growth Rates:\\n\\n\"\n",
        "    for state, rate in bottom_states:\n",
        "        result += f\"{state}: {rate:.2f}%\\n\"\n",
        "\n",
        "    return result\n",
        "\n",
        "# Create intervention recommendations\n",
        "def generate_recommendations(forecast_df, ts_df):\n",
        "    \"\"\"Generate policy recommendations based on analysis\"\"\"\n",
        "    if forecast_df is None or len(forecast_df) == 0 or ts_df is None:\n",
        "        return \"No data available. Please run the analysis first.\"\n",
        "\n",
        "    # Find states with below-average GDP\n",
        "    latest_year = ts_df['Year'].max()\n",
        "    latest_data = ts_df[ts_df['Year'] == latest_year]\n",
        "    avg_gdp = latest_data['GDP_Per_Capita'].mean()\n",
        "\n",
        "    below_avg_states = latest_data[latest_data['GDP_Per_Capita'] < avg_gdp]['State'].tolist()\n",
        "\n",
        "    # Get growth rates\n",
        "    growth_rates = {}\n",
        "    for state in forecast_df['State'].unique():\n",
        "        state_data = forecast_df[forecast_df['State'] == state].sort_values('Year')\n",
        "        if len(state_data) >= 2:\n",
        "            first_gdp = state_data['Forecasted_GDP'].iloc[0]\n",
        "            last_gdp = state_data['Forecasted_GDP'].iloc[-1]\n",
        "            growth = (last_gdp / first_gdp - 1) * 100\n",
        "            growth_rates[state] = growth\n",
        "\n",
        "    # Filter to only include below average states\n",
        "    below_avg_growth = {state: rate for state, rate in growth_rates.items() if state in below_avg_states}\n",
        "\n",
        "    # Sort by growth rate\n",
        "    sorted_states = sorted(below_avg_growth.items(), key=lambda x: x[1])\n",
        "\n",
        "    # Generate recommendations\n",
        "    result = \"## Policy Intervention Recommendations\\n\\n\"\n",
        "    result += \"### Priority States for Intervention:\\n\\n\"\n",
        "\n",
        "    for state, rate in sorted_states[:min(5, len(sorted_states))]:\n",
        "        result += f\"**{state}** - Current GDP below average, Projected Growth: {rate:.2f}%\\n\\n\"\n",
        "\n",
        "        # Customize recommendations based on growth rate\n",
        "        if rate < 0:\n",
        "            result += \"- **URGENT ACTION NEEDED**: Negative growth projection\\n\"\n",
        "            result += \"- Implement economic stimulus package\\n\"\n",
        "            result += \"- Develop infrastructure investment plan\\n\"\n",
        "            result += \"- Create special economic zones to attract investment\\n\\n\"\n",
        "        elif rate < 5:\n",
        "            result += \"- **HIGH PRIORITY**: Low growth projection\\n\"\n",
        "            result += \"- Focus on skill development programs\\n\"\n",
        "            result += \"- Provide tax incentives for new businesses\\n\"\n",
        "            result += \"- Improve transportation and logistics infrastructure\\n\\n\"\n",
        "        else:\n",
        "            result += \"- **MEDIUM PRIORITY**: Moderate growth but still below average\\n\"\n",
        "            result += \"- Enhance existing growth drivers\\n\"\n",
        "            result += \"- Target specific sectors with growth potential\\n\"\n",
        "            result += \"- Implement inclusive growth policies\\n\\n\"\n",
        "\n",
        "    result += \"### Overall Inequality Reduction Strategy:\\n\\n\"\n",
        "    result += \"1. **Progressive Investment Allocation**: Allocate development funds inversely proportional to GDP ranking\\n\"\n",
        "    result += \"2. **Targeted Skill Development**: Focus education and training programs in lower-GDP states\\n\"\n",
        "    result += \"3. **Infrastructure Equalization**: Prioritize connectivity and utilities in underserved regions\\n\"\n",
        "    result += \"4. **Industrial Incentives**: Provide stronger incentives for businesses to locate in developing states\\n\"\n",
        "    result += \"5. **Social Safety Net**: Expand social programs in areas with lowest GDP per capita\\n\"\n",
        "\n",
        "    return result\n",
        "\n",
        "# Define UI functions\n",
        "def handle_analysis(file):\n",
        "    global global_forecast_df, global_ts_df\n",
        "\n",
        "    if file is None:\n",
        "        return \"Please upload a file first.\", None, None, {}, None, None\n",
        "\n",
        "    status, model_results, metrics, viz_images, feature_importance, social_impact, forecast_df = run_analysis(file)\n",
        "\n",
        "    # Store the forecast_df for other functions to use\n",
        "    global_forecast_df = forecast_df\n",
        "\n",
        "    # Extract the visualizations\n",
        "    viz_dict = {}\n",
        "    if viz_images:\n",
        "        if 'top_states_trend' in viz_images:\n",
        "            viz_dict['top_states_trend'] = f\"data:image/png;base64,{viz_images['top_states_trend']}\"\n",
        "        if 'inequality_trend' in viz_images:\n",
        "            viz_dict['inequality_trend'] = f\"data:image/png;base64,{viz_images['inequality_trend']}\"\n",
        "        if 'model_performance' in viz_images:\n",
        "            viz_dict['model_performance'] = f\"data:image/png;base64,{viz_images['model_performance']}\"\n",
        "        if 'forecast' in viz_images:\n",
        "            viz_dict['forecast'] = f\"data:image/png;base64,{viz_images['forecast']}\"\n",
        "        if 'cv_trend' in viz_images:\n",
        "            viz_dict['cv_trend'] = f\"data:image/png;base64,{viz_images['cv_trend']}\"\n",
        "\n",
        "    return status, model_results, metrics, viz_dict, social_impact\n",
        "\n",
        "# Handle state prediction\n",
        "def handle_state_prediction(state_name):\n",
        "    global global_forecast_df\n",
        "\n",
        "    if global_forecast_df is None:\n",
        "        return \"Please run the analysis first to generate forecasts.\"\n",
        "\n",
        "    return predict_state(state_name, global_forecast_df)\n",
        "\n",
        "# Handle growth analysis\n",
        "def handle_growth_analysis():\n",
        "    global global_forecast_df\n",
        "\n",
        "    if global_forecast_df is None:\n",
        "        return \"Please run the analysis first to generate forecasts.\"\n",
        "\n",
        "    return get_growth_analysis(global_forecast_df)\n",
        "\n",
        "# Handle recommendations\n",
        "def handle_recommendations():\n",
        "    global global_forecast_df, global_ts_df\n",
        "\n",
        "    if global_forecast_df is None or global_ts_df is None:\n",
        "        return \"Please run the analysis first to generate forecasts.\"\n",
        "\n",
        "    return generate_recommendations(global_forecast_df, global_ts_df)\n",
        "\n",
        "# Fixed handle_analysis function with proper visualization output handling\n",
        "def handle_analysis(file):\n",
        "    global global_forecast_df, global_ts_df\n",
        "\n",
        "    if file is None:\n",
        "        return \"Please upload a file first.\", None, None, None, None, None, None, None, None\n",
        "\n",
        "    status, model_results, metrics, viz_images, feature_importance, social_impact, forecast_df = run_analysis(file)\n",
        "\n",
        "    # Store the forecast_df for other functions to use\n",
        "    global_forecast_df = forecast_df\n",
        "\n",
        "    # Initialize visualization outputs as None\n",
        "    top_states_viz = None\n",
        "    inequality_viz = None\n",
        "    model_perf_viz = None\n",
        "    forecast_viz = None\n",
        "    cv_viz = None\n",
        "\n",
        "    # Process visualization images if available\n",
        "    if viz_images and isinstance(viz_images, dict):\n",
        "        if 'top_states_trend' in viz_images:\n",
        "            top_states_viz = f\"data:image/png;base64,{viz_images['top_states_trend']}\"\n",
        "        if 'inequality_trend' in viz_images:\n",
        "            inequality_viz = f\"data:image/png;base64,{viz_images['inequality_trend']}\"\n",
        "        if 'model_performance' in viz_images:\n",
        "            model_perf_viz = f\"data:image/png;base64,{viz_images['model_performance']}\"\n",
        "        if 'forecast' in viz_images:\n",
        "            forecast_viz = f\"data:image/png;base64,{viz_images['forecast']}\"\n",
        "        if 'cv_trend' in viz_images:\n",
        "            cv_viz = f\"data:image/png;base64,{viz_images['cv_trend']}\"\n",
        "\n",
        "    return status, model_results, metrics, top_states_viz, inequality_viz, model_perf_viz, forecast_viz, cv_viz, social_impact\n",
        "\n",
        "# Create the Gradio interface with fixed connections\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=\"GDP Per Capita Analysis Tool\") as app:\n",
        "        gr.Markdown(\"# State Economic Analysis and Forecast Tool\")\n",
        "        gr.Markdown(\"Upload a CSV file with state GDP per capita data to analyze trends and generate forecasts.\")\n",
        "\n",
        "        with gr.Tab(\"Run Analysis\"):\n",
        "            with gr.Row():\n",
        "                file_input = gr.File(label=\"Upload CSV Data File\")\n",
        "\n",
        "            with gr.Row():\n",
        "                analyze_btn = gr.Button(\"Run Analysis\", variant=\"primary\")\n",
        "\n",
        "            with gr.Row():\n",
        "                status_output = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "            with gr.Accordion(\"Model Results\", open=False):\n",
        "                model_output = gr.Textbox(label=\"Model Evaluation\", interactive=False)\n",
        "                metrics_output = gr.Textbox(label=\"Best Model Metrics\", interactive=False)\n",
        "\n",
        "            with gr.Accordion(\"Visualizations\", open=True):\n",
        "                with gr.Row():\n",
        "                    top_states_img = gr.Image(label=\"GDP Trends for Top States\")\n",
        "                    inequality_img = gr.Image(label=\"Inequality Trend\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    model_perf_img = gr.Image(label=\"Model Performance\")\n",
        "                    forecast_img = gr.Image(label=\"GDP Forecast\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    cv_img = gr.Image(label=\"Coefficient of Variation Trend\")\n",
        "\n",
        "            with gr.Accordion(\"Social Impact Analysis\", open=False):\n",
        "                social_output = gr.Textbox(label=\"Social Impact Analysis\", interactive=False)\n",
        "\n",
        "        with gr.Tab(\"State Predictions\"):\n",
        "            with gr.Row():\n",
        "                state_input = gr.Textbox(label=\"Enter State Name\")\n",
        "                predict_btn = gr.Button(\"Get Prediction\", variant=\"primary\")\n",
        "\n",
        "            with gr.Row():\n",
        "                prediction_output = gr.Textbox(label=\"Prediction Results\", interactive=False)\n",
        "\n",
        "        with gr.Tab(\"Growth Analysis\"):\n",
        "            with gr.Row():\n",
        "                growth_btn = gr.Button(\"Analyze Growth Rates\", variant=\"primary\")\n",
        "\n",
        "            with gr.Row():\n",
        "                growth_output = gr.Textbox(label=\"Growth Analysis\", interactive=False)\n",
        "\n",
        "        with gr.Tab(\"Policy Recommendations\"):\n",
        "            with gr.Row():\n",
        "                recommendations_btn = gr.Button(\"Generate Recommendations\", variant=\"primary\")\n",
        "\n",
        "            with gr.Row():\n",
        "                recommendations_output = gr.Markdown(label=\"Policy Recommendations\")\n",
        "\n",
        "        # Connect functions to buttons with individual outputs clearly specified\n",
        "        analyze_btn.click(\n",
        "            fn=handle_analysis,\n",
        "            inputs=[file_input],\n",
        "            outputs=[\n",
        "                status_output,          # Status\n",
        "                model_output,           # Model Results\n",
        "                metrics_output,         # Metrics\n",
        "                top_states_img,         # Top States Trend Image\n",
        "                inequality_img,         # Inequality Trend Image\n",
        "                model_perf_img,         # Model Performance Image\n",
        "                forecast_img,           # Forecast Image\n",
        "                cv_img,                 # CV Trend Image\n",
        "                social_output           # Social Impact Analysis\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        predict_btn.click(\n",
        "            fn=handle_state_prediction,\n",
        "            inputs=[state_input],\n",
        "            outputs=[prediction_output]\n",
        "        )\n",
        "\n",
        "        growth_btn.click(\n",
        "            fn=handle_growth_analysis,\n",
        "            inputs=[],\n",
        "            outputs=[growth_output]\n",
        "        )\n",
        "\n",
        "        recommendations_btn.click(\n",
        "            fn=handle_recommendations,\n",
        "            inputs=[],\n",
        "            outputs=[recommendations_output]\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "# Add a debugging function for visualization issues\n",
        "def debug_visualizations(ts_df, y_test=None, y_pred=None, best_model_name=None, forecast_df=None):\n",
        "    \"\"\"Generate and debug visualizations to troubleshoot issues\"\"\"\n",
        "    try:\n",
        "        images = {}\n",
        "\n",
        "        # 1. Simple test visualization to verify plotting works\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot([1, 2, 3, 4], [10, 20, 30, 40], marker='o')\n",
        "        plt.title('Test Visualization')\n",
        "        plt.xlabel('X')\n",
        "        plt.ylabel('Y')\n",
        "        plt.grid(True)\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        plt.savefig(buf, format='png')\n",
        "        buf.seek(0)\n",
        "        test_img = base64.b64encode(buf.read()).decode('utf-8')\n",
        "        images['test_viz'] = test_img\n",
        "        plt.close()\n",
        "\n",
        "        # 2. Try the top states trend with extra error handling\n",
        "        if ts_df is not None and len(ts_df) > 0:\n",
        "            try:\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                latest_year = ts_df['Year'].max()\n",
        "                top_states_data = ts_df[ts_df['Year'] == latest_year]\n",
        "\n",
        "                if len(top_states_data) > 0:\n",
        "                    top_states = top_states_data.nlargest(min(5, len(top_states_data)), 'GDP_Per_Capita')['State'].unique()\n",
        "                    top_states_data = ts_df[ts_df['State'].isin(top_states)]\n",
        "\n",
        "                    if len(top_states_data) > 0:\n",
        "                        sns.lineplot(data=top_states_data, x='Year', y='GDP_Per_Capita', hue='State')\n",
        "                        plt.title('GDP Per Capita Trends for Top States')\n",
        "                        plt.xlabel('Year')\n",
        "                        plt.ylabel('GDP Per Capita')\n",
        "                        plt.grid(True, alpha=0.3)\n",
        "\n",
        "                        buf = io.BytesIO()\n",
        "                        plt.savefig(buf, format='png')\n",
        "                        buf.seek(0)\n",
        "                        images['top_states_trend'] = base64.b64encode(buf.read()).decode('utf-8')\n",
        "                plt.close()\n",
        "            except Exception as e:\n",
        "                print(f\"Error in top states visualization: {str(e)}\")\n",
        "\n",
        "        return images\n",
        "    except Exception as e:\n",
        "        print(f\"Debug visualization error: {str(e)}\")\n",
        "        return {}\n",
        "\n",
        "# Update run_analysis to include debugging\n",
        "def run_analysis_with_debug(file):\n",
        "    \"\"\"Wrapper around run_analysis with additional debugging\"\"\"\n",
        "    try:\n",
        "        result = run_analysis(file)\n",
        "        status, model_results, metrics, viz_images, feature_importance, social_impact, forecast_df = result\n",
        "\n",
        "        # If visualizations failed, try to debug\n",
        "        if viz_images is None or len(viz_images) == 0:\n",
        "            print(\"Visualizations failed, running debug function...\")\n",
        "            if 'global_ts_df' in globals() and global_ts_df is not None:\n",
        "                debug_viz = debug_visualizations(global_ts_df, forecast_df=forecast_df)\n",
        "                if debug_viz and len(debug_viz) > 0:\n",
        "                    viz_images = debug_viz\n",
        "\n",
        "        return status, model_results, metrics, viz_images, feature_importance, social_impact, forecast_df\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        return f\"Error in analysis with debug: {str(e)}\\n{traceback.format_exc()}\", None, None, None, None, None, None\n",
        "\n",
        "# Modified main function\n",
        "def main():\n",
        "    # Enable debug messages\n",
        "    import logging\n",
        "    logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "    # Create and launch the interface\n",
        "    app = create_interface()\n",
        "    app.launch(share=True)  # share=True will generate a public URL using ngrok\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1yTLOkRGdgEp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
